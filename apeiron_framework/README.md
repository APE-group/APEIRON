APEIRON Example Design
================================================================================

APEIRON is a framework based on the Vitis High-Level Synthesis (HLS) from Xilinx
and on the APEnet interconnect IPs from INFN APE Lab. 
The APEnet Switch/Router IPs allows direct communication between HLS
Tasks both on the same FPGA device or on a different device interconnected with
sub-microsecond latency through the APElink serial channel.


.Pre-requisites

- Xilinx Alveo U200 card 
(https://www.xilinx.com/products/boards-and-kits/alveo/u200.html)

- Xilinx Vitis 2021.1 
(https://www.xilinx.com/support/download/index.html/content/xilinx/en/downloadNav/vitis/2021-1.html)

- Xilinx runtime (XRT), XDMA Deployment Target Platform, and XDMA Development Target Platform
(https://www.xilinx.com/products/boards-and-kits/alveo/u200.html#gettingStarted)


.Environment

> source /opt/Xilinx/Vitis/2021.1/settings64.sh

> source /opt/xilinx/xrt/setup.sh

.Makefile

Choose the $TARGET_APP from the /dev_apps/ directory and write it in the Makefile.


DEV_APPS/LAT_BW
================================================================================

.Latency and Bandwidth tests Design

This example design demonstrates the main functionalities of the framework, 
using an APEnet Switch/Router configured with 2 intranode ports and 1 internode 
port.
Each port is bidirectional, each direction sports a header/data FIFO
couple according the APEnet packet protocol.

Port 0 of the switch is directly connected to the krnl_sendreceive() HLS kernel

Port 1 of the switch is connected to the krnl_sr() HLS
kernel, through the autogenerated dispatcher_1() and
aggregator_1().

The example kernel, connected with the HAPECOM settings, is defined as:

void krnl_sr(
		header_stream_t message_hdr_in[N_INPUT_CHANNELS],
		message_stream_t message_data_in[N_INPUT_CHANNELS],

		header_stream_t message_hdr_out[N_OUTPUT_CHANNELS],
		message_stream_t message_data_out[N_OUTPUT_CHANNELS]
		)

having N_INPUT_CHANNELS and N_OUTPUT_CHANNELS to receive/send
incoming/outgoing messages.
So the dispatcher...() and aggregator...() kernels work as adaptors
from and toward the single bidir channel of the switch port.

The host application orchestrates the execution of the test,
initializing the send/receive buffers in the device global memory and
launching the HLS kernels.

In the latency_test, a packet is sent from the node_0:port_0 to node_1:port_0 where it is received and then sent back.
This operation is repeated for 1M packets, in order to reduce the overhead of the host XRT calls in the latency calculation.
In the bandwidth_test, 10000 packets are sent from the node_0:port_0 to node_1:port_0 where they are all received.
After the reception, an ACK packet is sent back to the first node, in order to calculate the time between the start and 
the end of the process of sending, and so the bandiwidth.

The user defined kernels, connected via HAPECOM Communication protocol, have to declare their number of input and
output channels and the switch port they are connected to in the 
config.yaml configuration file.
In this example:

kernels:
  - name: krnl_sr
    input_channels: 4
    output_channels: 4
    switch_port: 1


.Build steps

First step is to generate the vpp_linker.cfg Vitis project
configuration file, that specifies the target device, the operational
clock frequency and the interconnections between the components:

> ./generate.py 
Generating dispatcher/aggregator for kernel krnl_sr
Generating a dispatcher with 4 output channels...
Generating an aggregator with 4 input channels...
Generating linker...

After this, it is possible to launch the build project (this takes a
couple of hours at least, refer to make.log file to inspect a
successfully build process).

> make

.Execution

To launch the test with default parameters:

> ./sh_tests/loop.sh
> ./sh_tests/bandwidth_loop.sh
> ./sh_tests/bw_nodetonode.sh $node_coord $packet_size



DEV_APPS/CNN_APEIRON_PREPROCESSING_NODE
================================================================================

.PREPROCESSING_NODE

Port 0 of the switch is connected to krnl_sender() and krnl_receiver() HLS kernels,
via respectively the autogenerated dispatcher_0() and aggregator_0()

Port 1 of the switch is connected to the image_sender() HLS
kernel, through the autogenerated dispatcher_1() and
aggregator_1().
This kernel is the main part of the preprocessing node, since it has the task to receive
from krnl_sender() data packets containing physical single event informations each
(mgp packets), and to process them in order to generate the input image to be sent through
the network to the Computing nodes where the CNN HLS kernels are implemented. 

Image_sender(), connected with the HAPECOM settings, is defined as:

void image_sender(
		unsigned nports,
		unsigned nboards,
		message_stream_t message_data_in[N_INPUT_CHANNELS],
		message_stream_t message_data_out[N_OUTPUT_CHANNELS]
		)

having N_INPUT_CHANNELS and N_OUTPUT_CHANNELS to receive/send
incoming/outgoing messages.
From the host application is possible to modify the nports and nboards kernel parameters:
this will change the execution setup, choosing to work with #nports CNN kernel on #nboards boards. 

The host application orchestrates the execution of the test,
initializing the send/receive buffers (containing, respectively, physical event data and CNNs results)
in the device global memory and launching the HLS kernels.

In this example:

kernels:
  - name: krnl_sender
    input_channels: 0
    output_channels: 1
    switch_port: 0

  - name: krnl_receiver
    input_channels: 1
    output_channels: 0
    switch_port: 0

  - name: image_sender
    input_channels: 1
    output_channels: 0
    switch_port: 0


.Build steps

First step is to generate the vpp_linker.cfg Vitis project
configuration file, that specifies the target device, the operational
clock frequency and the interconnections between the components:

> ./generate.py 
Generating dispatcher/aggregator for kernel krnl_sr
Generating a dispatcher with 4 output channels...
Generating an aggregator with 4 input channels...
Generating linker...

After this, it is possible to launch the build project (this takes a
couple of hours at least, refer to make.log file to inspect a
successfully build process).

> make

.Execution

To launch the test with default parameters:

> ./host_test -b preprocessing_node.xclbin -n #npackets


 
     
DEV_APPS/CNN_APEIRON_COMPUTING_NODE
================================================================================

.COMPUTING_NODE

Port 0 and 1 of the switch is connected to 2 top_nnet() HLS kernels,
via respectively the autogenerated dispatcher_0/1() and aggregator_0/1()

This kernel is the main part of the computing node, since it has the task to receive
from image_sender() data packets containing CNN input images each, and to process them 
with a CNN computing in order to generate single event predictions to be sent through
the network back to the Preprocessing node where krnl_receiver() is implemented. 

The host application orchestrates the execution of the test,
launching the HLS kernels and setting the node x-coordinate.

In this example:

kernels:
  - name: top_nnet
    input_channels: 1
    output_channels: 1
    switch_port: 0

  - name: top_nnet
    input_channels: 1
    output_channels: 1
    switch_port: 1


.Build steps

First step is to generate the vpp_linker.cfg Vitis project
configuration file, that specifies the target device, the operational
clock frequency and the interconnections between the components:

> ./generate.py 
Generating dispatcher/aggregator for kernel krnl_sr
Generating a dispatcher with 4 output channels...
Generating an aggregator with 4 input channels...
Generating linker...

After this, it is possible to launch the build project (this takes a
couple of hours at least, refer to make.log file to inspect a
successfully build process).

> make

.Execution

To launch the test with default parameters:

> ./host_test -b computing_node.xclbin -n #node_coordinate


